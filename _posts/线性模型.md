---
title: 线性模型
tags:
  - MachineLearning
abbrlink: 1376188223
date: 2017-04-06 17:50:27
---
## 基本形式
![](https://raw.githubusercontent.com/necusjz/p/master/MachineLearning/3_1.jpeg)
xi 是 x 在第 i 个属性上的取值。
w 和 b 学得之后，模型就得以确定。
许多功能更为强大的非线性模型（nolinear model）可在线性模型的基础上，通过引入**层级结构**或**高维映射**而得。
w 直观表达了各属性在预测中的重要性，因此线性模型有很好的可解释性（comprehensibility）。
<!--more-->
## 线性回归（linear regression）
试图学得一个线性模型以尽可能准确地预测实值输出标记。
* 若属性值间存在序（order）关系，可通过连续化将其转化为连续值。如：高度的取值高、中、低，可转化为 {1.0, 0.5, 0.0}。
* 若属性值间不存在序关系，通常转化为 k 维向量。如：瓜类的取值西瓜、南瓜、黄瓜，可转化为：(0, 0, 1)、(0, 1, 0)、(1, 0, 0)。

将无序属性连续化，会不恰当地引入序关系，对后续处理如距离计算等造成误导。
基于**均方误差**最小化来进行模型求解的方法称为**最小二乘法**（least square method）。
最小二乘法，试图找到一条直线，使所有样本到直线的**欧氏距离**（Eucidean distance）之和最小。
多元线性回归（multivariate linear regression）模型为：
![](https://raw.githubusercontent.com/necusjz/p/master/MachineLearning/3_2.jpeg)
XTX 往往不是满秩矩阵，因变量过多，会解出多组解都能使均方误差最小化。选择哪一个解作为输出，将由学习算法的**归纳偏好**决定，常见的做法是引入正则化（regularization）。
广义线性模型（generalized linear model）：
![](https://raw.githubusercontent.com/necusjz/p/master/MachineLearning/3_3.jpeg)
单调可微函数 g(·) 称为**联系函数**（link function），对数线性回归是广义线性模型在 g(·)=ln(·) 时的特例。
广义线性模型的参数估计（parameter estimation）常通过最小二乘法或**极大似然法**进行。
## 对数几率回归
对于分类任务，只需找一个单调可微函数将真实标记 y 与线性回归模型的预测值联系起来。
**单位阶跃函数**（unit-step function）不连续，选择近似的**对数几率函数**（logistic function）作为**替代函数**（surrogate function）。
对率函数是 Sigmoid 函数最重要的代表
将 y 视为样本x作为正例的可能性，则 1-y 是其反例可能性。两者的比值 y/(1-y) 称为**几率**（odds），反映了x作为正例的相对可能性，取对数得到**对数几率**（log odds）：ln[y/(1-y)]。
对率回归实际是一种分类学习方法（优点）：
* 避免了假设分布不准确带来的问题
* 对许多需利用概率辅助决策的任务很有用
* 对率函数是任意阶可导的凸函数，许多**数值优化算法**都可直接用于求取最优解。

经典的数值优化算法：**梯度下降法**（gradient descent method）、**牛顿法**（Newton method）。
## 线性判别分析
线性判别分析（Linear Discriminant Analysis），亦称 Fisher 判别分析。
LDA 的思想非常朴素，设法将样例投影到一条直线上：
* 使得同类样例的投影点尽可能接近，投影点的**协方差**尽可能小。
* 使得异样样例的投影点尽可能远离，让类中心之间的**距离**尽可能大。
![](https://raw.githubusercontent.com/necusjz/p/master/MachineLearning/3_4.jpeg)

定义：
* 类内散度矩阵（within-class scatter matrix）：Sw
* 类间散度矩阵（between-class scatter matrix）：Sb

LDA 欲最大化的目标，即 Sb 与 Sw 的**广义瑞利商**（generalized Rayleigh quotient）。
考虑到数值解的稳定性，在实践中通常是对 Sw 进行奇异值分解。
LDA 可从**贝叶斯决策理论**的角度阐释并证明。当两类数据同先验、满足高斯分布且协方差相等时，LDA 可达到最优分类。
可以将 LDA 推广到多分类任务中，多分类 LDA 将样本投影到 d' 维空间，d' 通常远小于数据原有的属性数 d。因此 LDA 也常被视为一种经典的监督降维技术。
## 多分类学习
### 基本思路
**拆解法**，将多分类任务拆为若干个二分类任务求解：
* 如何对多分类任务进行拆分
* 如何对多个分类器（classifier）进行集成

### 拆分策略
最经典的拆分策略有三种：
1. 一对一（OvO）：产生 N(N-1)/2 个分类任务，把预测得最多的类别作为最终分类结果。
2. 一对其余（OvR）：训练 N 个分类器，若有多个分类器预测为正类，选择置信度最大的类别标记作为分类结果。
3. 多对多（MvM）：OvO、OvR 是 MvM 的特例，正、反类构造必须有特殊的设计。例如，**纠错输出码**（Error Correcting Output Codes）。
![](https://raw.githubusercontent.com/necusjz/p/master/MachineLearning/3_5.jpeg)

OvO 的存储开销和测试时间开销通常比 OvR 更大，在类别很多时，OvO 的训练时间开销通常比 OvR 更小。预测性能，多数情况下两者差不多。
### 纠错输出码
ECOC 工作过程主要分为两步：
* 编码：将 N 个类别做M次划分，训练出 M 个分类器。
* 解码：将预测编码与每个类别各自的编码进行比较，返回其中距离最小的类别作为最终预测结果。

类别划分通过**编码矩阵**（coding matrix）：
* 二元码：正类、反类；
* 三元码：正类、反类、停用类；

ECOC 编码越长，纠错能力越强，意味着所需训练的分类器越多，计算、存储开销都会增大。
由于类别数有限，可能的组合数目也是有限的，码长超过一定范围后就失去了意义。
**同等长度**的编码，两个类别之间的编码距离越远，则纠错能力越强。通常并不需获得理论最优编码（原因）：
* 非最优编码在实践中往往已能产生足够好的分类器。
* 机器学习问题涉及很多因素，不同拆解方式导致的二分类问题的难度不同。

## 类别不平衡问题
类别不平衡（class-imbalance）就是指分类任务中不同类别的训练样例数目差别很大的情况。
### 基本策略
类别不平衡学习的一个基本策略——**再缩放**（rescaling）：
![](https://raw.githubusercontent.com/necusjz/p/master/MachineLearning/3_6.jpeg)
**无偏采样**意味着，真实样本总体的类别比例在训练集中得以保持。
### 再缩放
再缩放的三类做法：
* 对训练集里的反类样例进行**欠采样**（undersampling），去除一些反例。避免丢失一些重要信息，代表性算法 EasyEnsemble。
* 对正类样例进行**过采样**（oversampling），增加一些正例。避免招致严重的过拟合，代表性算法 SMOTE。
* 直接基于原始训练集进行学习，预测时，将式 (3.48) 嵌入到决策过程中，称为**阈值移动**（threshold-moving）。

再缩放是**代价敏感学习**（cost-sensitive learning）的基础。
## 阅读材料
非均等代价和类别不平衡性虽然都可借助**再缩放**技术，但两者本质不同。
类别不平衡学习中通常是较小类的代价更高，否则无需进行特殊处理。
多分类学习中虽然有多个类别，但每个样例仅属于一个类别。如果希望一个样本同时预测出多个类别标记，这样的任务是**多标记学习**（multi-label learning）。

[习题答案](http://blog.csdn.net/icefire_tyh/article/details/52069025)
