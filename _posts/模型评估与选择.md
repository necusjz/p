---
title: 模型评估与选择
date: 2017-03-29 18:44:53
tags:
  - MachineLearning
---
## 训练误差与过拟合
精度（accuracy），精度 = 1-错误率。
* 在训练集上的误差称为**训练误差**（training error）
* 在新样本上的误差称为**泛化误差**（generalization error）

实际希望的，是在新样本上能表现得很好的学习器。
* 过拟合（overfitting）：把训练样本自身的一些特点，当作了所有潜在样本都会具有的一般性质。
* 欠拟合（underfitting）：对训练样本的一般性质尚未学好。

过拟合是**无法彻底避免**的。若可彻底避免过拟合，则通过经验误差最小化就能获得最优解。
## 评估方法
以测试集的**测量误差**（testing error）作为泛化误差的近似。
测试集应该尽可能与训练集互斥，通过对**数据集 D** 进行适当的处理，从中产生**训练集 S** 和**测试集 T**。
<!--more-->
### 留出法（hold-out）
直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集 S，另一个作为测试集 T。
尽可能保持数据**分布**的一致性，例如在分类任务中至少要保持样本的类别比例相似。
保留类别比例的采样方式通常称为**分层采样**（stratified sampling）。
一般要采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果。
测试集小时，评估结果的**方差**较大；训练集小时，评估结果的**偏差**较大。
常见做法是将大约 2/3～4/5 的样本用于训练，测试集至少应含 30 个样例。
### 交叉验证法（cross validation）
将数据集 D 划分为 k 个大小相似的互斥子集，每次用 k-1 个子集的并集作为训练集，余下的那个子集作为测试集。
**k 折**（k-fold）交叉验证，k 的取值通常 5、10、20 等。
使用不同的划分重复 p 次
> "10 次 10 折交叉验证法"与“100 次留出法”都是进行了 100 次训练、测试。

**留一法**（Leave-One-Out）：数据集 D 中包含 m 个样本，k = m。
* 优点：训练集与初始数据集相比只少了一个样本，评估结果往往被认为比较准确。
* 缺点：在数据集比较大时，训练 m 个模型的计算开销可能是难以忍受的。

### 自助法（bootstrapping）
自助采样亦称“可重复采样”或“有放回采样”。
初始数据集 D 中约有 36.8%（1/e）的样本未出现在采样数据集 D' 中，将 D\D' 用作测试集，这样的测试结果，亦称**包外估计**（out-of-bag estimate）。
在数据集较小、难以有效划分训练、测试集时很有用。
改变了初始数据集的分布，会引入估计偏差。在初始数据量足够时，留出法和交叉验证更常用一些。
### 调参与最终模型
两类**参数：**
* 算法的参数：亦称超参数，由人工设定，数目常在 10 以内。
* 模型的参数：通过学习来产生，数目可能很多。

对每个参数选定一个**范围**和变化**步长**，参数调得好不好往往对最终模型性能有关键性影响。
学习算法和参数配置已选定后，应该用数据集 D 重新训练模型。使用所有 m 个样本，这才是最终提交给用户的模型。
把训练数据另外划分为训练集和验证集（validation set），基于验证集上的性能进行模型选择和调参。
## 性能度量
不仅需要有效可行的实验估计方法，还需要有衡量模型泛化能力的评价标准，即性能度量（performance measure）。
什么样的模型是好的，不仅取决于算法和数据，还决定于任务需求。
回归任务最常用的性能度量是，均方误差（mean squared error）。
### 错误率与精度
### 查准率、查全率与 F1
查准率亦称“准确率”，查全率亦称“召回率”。
查准率和查全率是一对矛盾的度量
查准率-查全率曲线（P-R 曲线）：
* 若一个学习器的 P-R 曲线被另一个学习器的曲线完全包住，后者性能优于前者。
* 交叉时，比较面积，可借助于平衡点（Break-Even Point），即“查准率 = 查全率”时的取值。 
![](https://raw.githubusercontent.com/umarellyh/mPOST/master/MachineLearning/2_1.jpeg)

BEP 还是过于简化了些，更常用的是 F1 度量。F1 是基于查准率和查全率的调和平均（harmonic mean）定义的，调和平均更重视较小值。
有多个二分类混淆矩阵：
* 宏（macro）～：先在各混淆矩阵上分别计算，再计算平均值。
* 微（micro）～：先将各混淆矩阵的对应元素进行平均，再计算。 

### ROC 与 AUC
将预测值与一个分类阈值（threshold）进行比较，大于阈值则为正类。
“最可能”是正例的排在最前面，以某个截断点（cut point）将样本分成两部分。若更重视查准率，则选择靠前的位置进行截断。
排序本身的质量好坏，体现了“一般情况下”泛化性能的好坏。
ROC 全称是**受试者工作特征**（Receiver Operating Characteristic）曲线，纵轴是**真正例率**（TPR），横轴是**假正例率**（FPR）。
点 (0, 1) 对应于将所有正例排在所有反例之前的理想模型。
若 ROC 曲线发生交叉，较为合理的判据是比较 ROC 曲线下的面积，即 AUC（Area Under ROC Curve）。
排序损失（loss）：
* 若正例的预测值小于反例，则记一个“罚分”，若相等，则记 0.5 个。
* 对应 ROC 曲线之上的面积，AUC = 1-lrank。 

### 代价敏感错误率与代价曲线
为权衡不同类型错误所造成的不同损失，可为错误赋予**非均等代价**（unequal cost）。
根据任务的领域知识设定一个**代价矩阵**（cost matrix），cost ij 表示将第 i 类样本预测为第 j 类样本的代价。
以二分类任务为例，损失程度相差越大，cost 01 与 cost 10 值的差别越大，重要的是代价比值而非绝对值。
在非均等代价下，ROC 曲线不能直接反映出学习器的期望总体代价，**代价曲线**（cost curve）则可达到该目的。
规范化（normalization）是将不同变化范围的值映射到相同的固定范围中。常见的是 [0, 1]，此时亦称**归一化**。
在代价平面上绘制一条从 (0, FPR) 到 (1, FNR) 的线段，取所有线段的下界，围成的面积即为，在所有条件下学习器的期望总体代价。
![](https://raw.githubusercontent.com/umarellyh/mPOST/master/MachineLearning/2_2.jpeg)
## 比较检验
### 机器学习中性能的比较很复杂
希望比较的是泛化性能，通过实验评估方法获得的是测试集上的性能，两者对比结果可能未必相同。
测试集上的性能与测试集本身的选择有很大关系（大小、样例）。
机器学习算法本身有一定的随机性。
统计假设检验（hypothesis test）为学习器性能的比较提供了重要依据（默认以错误率为性能度量）。
### 假设检验
1-α 反映了结论的置信度（confidence）。
s.t. 是“subject to”的简写，使左边式子在右边条件满足时成立。
### 交叉验证 t 检验
若两个学习器的性能相同，则使用相同的训练集、测试集得到的测试错误率应相同。
进行有效假设检验的**重要前提：**测试错误率均为泛化错误率的独立采样。
实际上，不同轮次的训练集会有一定程度的重叠，使得测试错误率并不独立，导致过高估计假设成立的概率。为缓解这一问题，可采用 **5 X 2交叉验证**（5 次二折交叉验证）。
### McNemar 检验
标准正态分布变量的平方，即卡方分布。
### Friedman 检验与 Nemenyi 后续检验
在对一组数据集上**对多个算法进行比较**
每个数据集上列出两两比较的结果。
使用基于算法排序的 Friedman 检验。
* 若算法性能相同，则它们的平均序值应当相同。
* 若算法的性能显著不同，需进行后续检验（post-hoc test）。

## 偏差与方差
### 偏差—方差分解
偏差—方差分解（bias-variance decompositon）是解释学习算法泛化性能的一种重要工具。
泛化误差可分解为，偏差、方差与噪声之和。
* 偏差：刻画了学习算法本身的拟合能力。
* 方差：刻画了数据扰动所造成的影响。
* 噪声：刻画了学习问题本身的难度。

### 偏差—方差窘境
偏差—方差窘境（bias-variance dilemma）：很多学习算法都可控制**训练程度**。
* 决策树：控制层数。
* 神经网络：控制训练轮数。
* 集成学习方法：控制基学习器个数。

![](https://raw.githubusercontent.com/umarellyh/mPOST/master/MachineLearning/2_3.jpeg)
## 阅读材料
对分类任务，由于 0/1 损失函数的跳变性，推导出偏差-方差分解很困难。

[习题答案](http://blog.csdn.net/icefire_tyh/article/details/52065867)
