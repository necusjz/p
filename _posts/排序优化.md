---
title: 排序优化
date: 2020-12-30 23:43:52
tags:
  - CLRS
---
几乎所有的编程语言都会提供排序函数，比如 C 语言中 qsort()，C++ STL 中的 sort()、stable_sort()，还有 Java 语言中的 Collections.sort()。在平时的开发中，我们也都是直接使用这些现成的函数来实现业务逻辑中的排序功能。那你知道这些排序函数是如何实现的吗？

## 如何选择合适的排序算法？
如果要实现一个通用的、高效率的排序函数，我们应该选择哪种排序算法？我们先回顾一下前面讲过的几种排序算法：
![](https://raw.githubusercontent.com/was48i/mPOST/master/CLRS/geek/67.png)

我们前面讲过，线性排序算法的时间复杂度比较低，适用场景比较特殊。所以如果要写一个通用的排序函数，**不能选择线性排序算法**。如果对小规模数据进行排序，可以选择时间复杂度是 O(n^2) 的算法；如果对大规模数据进行排序，时间复杂度是 O(nlogn) 的算法更加高效。所以，为了兼顾任意规模数据的排序，一般都会**首选时间复杂度是 O(nlogn) 的排序算法来实现排序函数**。
<!--more-->

时间复杂度是 O(nlogn) 的排序算法不止一个，我们已经讲过的有归并排序、快速排序，后面讲堆的时候我们还会讲到堆排序。**堆排序和快速排序都有比较多的应用**，比如 Java 语言采用堆排序实现排序函数，C 语言使用快速排序实现排序函数。

## 如何优化快速排序？
如果数据原来就是有序的或者接近有序的，每次分区点都选择最后一个数据，那快速排序算法就会变得非常糟糕，时间复杂度就会退化为 O(n^2)。实际上，这种 O(n^2) 时间复杂度出现的主要原因还是因为我们**分区点选得不够合理**。最理想的分区点是：**被分区点分开的两个分区中，数据的数量差不多**。

### 三数取中法
我们从区间的首、尾、中间，分别取出一个数，然后**对比大小，取这 3 个数的中间值作为分区点**。这样每间隔某个固定的长度，取数据出来比较，将中间值作为分区点的分区算法，肯定要比单纯取某一个数据更好。但是，如果要排序的数组比较大，那“三数取中”可能就不够了，可能要“五数取中”或者“十数取中”。

### 随机法
随机法就是每次从要排序的区间中，**随机选择一个元素作为分区点**。这种方法并不能保证每次分区点都选的比较好，但是从概率的角度来看，也不大可能会出现每次分区点都选得很差的情况，所以平均情况下，这样选的分区点是比较好的。

为了避免快速排序里，递归过深而堆栈过小，导致堆栈溢出，我们有两种解决办法：
1. 限制递归深度。一旦递归过深，超过了我们事先设定的阈值，就停止递归；
2. 通过在堆上模拟实现一个函数调用栈，**手动模拟递归压栈、出栈的过程**，这样就没有了系统栈大小的限制；

## 举例分析排序函数
为了让你对如何实现一个排序函数有一个更直观的感受，我拿 Glibc 中的 qsort() 函数举例说明一下。如果你去看源码，你就会发现，**qsort() 会优先使用归并排序来排序输入数据**，因为归并排序的空间复杂度是 O(n)，所以对于小数据量的排序，比如 1KB、2KB 等，归并排序额外需要 1KB、2KB 的内存空间，这个问题不大。现在计算机的内存都挺大的，我们很多时候追求的是速度。

**要排序的数据量比较大的时候，qsort() 会改为用快速排序算法来排序**。qsort() 选择分区点的方法就是“三数取中法”，还有我们前面提到的递归太深会导致堆栈溢出的问题，qsort() 是通过自己实现一个堆上的栈，手动模拟递归来解决的。

实际上，qsort() 并不仅仅用到了归并排序和快速排序，它还用到了插入排序。在快速排序的过程中，当要排序的区间中，元素的个数小于等于 4 时，qsort() 就退化为插入排序，不再继续用递归来做快速排序，因为我们前面也讲过，**在小规模数据面前，O(n^2) 时间复杂度的算法并不一定比 O(nlogn) 的算法执行时间长**。

在大 O 复杂度表示法中，我们会省略低阶、系数和常数，也就是说，O(nlogn) 在没有省略低阶、系数、常数之前可能是 O(knlogn + c)，而且 k 和 c 有可能还是一个比较大的数。假设 k=1000，c=200，当我们对小规模数据（比如 n=100）排序时，n^2 的值实际上比 knlogn+c 还要小：
```cpp
knlogn+c = 1000 * 100 * log100 + 200 // 远大于 10000

n^2 = 100*100 = 10000
```
